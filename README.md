# üì¶ Run LLMs Locally on an NVIDIA GPU with a Web UI

> Custom Open-WebUI configuration with Ollama in Docker containers for running large‚Äëlanguage‚Äëmodels locally on an NVIDIA GPU with a simple web UI.
Accessible from all devices on your local network.
> All you need is a CUDA‚Äëcapable Linux box (or Docker‚Äëpowered Windows/Mac with WSL‚Äë2).

---

## üìã Prerequisites

- System with [CUDA-capable GPU](https://developer.nvidia.com/cuda-gpus).
- Have Docker and Docker Compose installed.
- Have the CUDA Toolkit correctly installed. ([Documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/))
- Have the NVIDIA Container Toolkit installed. ([Documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/))

---

## üöÄ Quickstart

### 1Ô∏è‚É£ Clone repository

```bash
git clone https://github.com/StinoBytes/local-llm.git
cd local-llm
```

### 2Ô∏è‚É£ Folder creation for persistency

```bash
mkdir -p data/ollama data/openwebui
```

```bash
sudo chown -R $(id -u):$(id -g) data
```

### 3Ô∏è‚É£ Spin up containers

```bash
docker compose up -d
```

### 4Ô∏è‚É£ Configure Open WebUI and add models

1. Open WebUI is accessible on [port 3000](http://localhost:3000).
2. Create admin account in the web interface.
3. Go to [admin panel > settings > models](http://localhost:3000/admin/settings/models) to add and download models from [Ollama's library](https://ollama.com/library).
4. Enjoy!

Optional: Add extra's according to the [Open WebUI Documentation](https://docs.openwebui.com/) such as integrating web search for models.

---

## üì¶ What‚Äôs actually running?

| SERVICE    | IMAGE                              | GPU support |
| ---------- | ---------------------------------- | ----------- |
| Ollama     | ollama/ollama:latest               | CUDA        |
| Open WebUI | ghcr.io/open-webui/open-webui:cuda | CUDA        |

Both images are built with the CUDA‚Äëruntime (nvidia/cuda:12.3.1-cudnn8-runtime-ubuntu22.04) and the NVIDIA Container Toolkit so that GPU memory is visible inside the containers.

    ‚ö†Ô∏è If you want to run on a non‚ÄëGPU machine, replace the Open WebUI 'image:' line in docker-compose.yml with the non‚ÄëGPU variant (openwebui/openwebui:latest) and drop the device_requests block in the Ollama service. The containers will still start but will run in CPU‚Äëonly mode and will be far slower.

---

## üìö Troubleshooting

| Symptom                                                   | Likely cause                                     | Fix                                                                                                       |
| --------------------------------------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------- |
| Containers fail to start: ‚Äúno such device /dev/nvidiactl‚Äù | NVIDIA drivers / toolkit not installed correctly | Re‚Äërun nvidia-setup from the toolkit docs; make sure the `nvidia-container-toolkit` package is installed. |
| Ollama logs show ‚ÄúCUDA error: not initialized‚Äù            | GPU driver isn‚Äôt exposed to Docker               | Install NVIDIA Container Toolkit and restart the Docker daemon (`sudo systemctl restart docker`).         |
| Data dirs stay empty after restart                        | Permissions issue                                | `sudo chown -R $(id -u):$(id -g) data` or add the user to the docker group.                               |
| UI doesn‚Äôt open                                           | Port 3000 blocked by firewall                    | `sudo ufw allow 3000` (or the firewall you use).                                                          |
| Models never download                                     | Network restrictions (e.g., corporate proxy)     | Set `HTTP_PROXY` / `HTTPS_PROXY` env vars in `docker-compose.yml`.                                        |

---

## üìÑ License

This project is released under the MIT License ‚Äì see the `LICENSE` file for details.

---
