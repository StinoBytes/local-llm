# ğŸ“¦ Run LLMs locally with Web UI on an NVIDIA GPU

> Custom Open-WebUI configuration with Ollama in Docker containers for running largeâ€‘languageâ€‘models locally on an NVIDIA GPU with a simple web UI.
Accessible from all devices on your local network.
> All you need is a CUDAâ€‘capable Linux box (or Dockerâ€‘powered Windows/Mac with WSLâ€‘2).

---

## ğŸ“‹ Prerequisites

- System with [CUDA-capable GPU](https://developer.nvidia.com/cuda-gpus).
- Have Docker and Docker Compose installed.
- Have the CUDA Toolkit correctly installed. ([Documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/))
- Have the NVIDIA Container Toolkit installed. ([Documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/))

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Clone repository

```bash
git clone https://github.com/StinoBytes/local-llm.git
cd local-llm
```

### 2ï¸âƒ£ Folder creation for persistency

```bash
mkdir -p data/ollama data/openwebui
```

```bash
sudo chown -R $(id -u):$(id -g) data
```

### 3ï¸âƒ£ Spin up containers

```bash
docker compose up -d
```

### 4ï¸âƒ£ Configure Open WebUI and add models

1. Open WebUI is accessible on [port 3000](http://localhost:3000).
2. Create admin account in the web interface.
3. Go to [admin panel > settings > models](http://localhost:3000/admin/settings/models) to add and download models from [Ollama's library](https://ollama.com/library).
4. Enjoy!

Optional: Add extra's according to the [Open WebUI Documentation](https://docs.openwebui.com/) such as integrating web search for models.

---

## ğŸ“¦ Whatâ€™s actually running?

| SERVICE    | IMAGE                              | GPU support |
| ---------- | ---------------------------------- | ----------- |
| Ollama     | ollama/ollama:latest               | CUDA        |
| Open WebUI | ghcr.io/open-webui/open-webui:cuda | CUDA        |

Both images are built with the CUDAâ€‘runtime and the NVIDIA Container Toolkit so that GPU memory is visible inside the containers.

> [!TIP]
> If you want to run on a nonâ€‘GPU machine, replace the Open WebUI `image:` line in `docker-compose.yml` with the nonâ€‘GPU variant (`openwebui/openwebui:latest`) and drop the `device_requests:` block in the Ollama service. The containers will still start, however they will run in CPUâ€‘only mode and will be far slower.

---

## ğŸ“š Troubleshooting

| Symptom                                                   | Likely cause                                     | Fix                                                                                                       |
| --------------------------------------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------- |
| Containers fail to start: â€œno such device /dev/nvidiactlâ€ | NVIDIA drivers / toolkit not installed correctly | Reâ€‘run nvidia-setup from the toolkit docs; make sure the `nvidia-container-toolkit` package is installed. |
| Ollama logs show â€œCUDA error: not initializedâ€            | GPU driver isnâ€™t exposed to Docker               | Install NVIDIA Container Toolkit and restart the Docker daemon (`sudo systemctl restart docker`).         |
| Data dirs stay empty after restart                        | Permissions issue                                | `sudo chown -R $(id -u):$(id -g) data` or add the user to the docker group.                               |
| UI doesnâ€™t open                                           | Port 3000 blocked by firewall                    | `sudo ufw allow 3000` (or the firewall you use).                                                          |
| Models never download                                     | Network restrictions (e.g., corporate proxy)     | Set `HTTP_PROXY` / `HTTPS_PROXY` env vars in `docker-compose.yml`.                                        |

---

## ğŸ“„ License

This project is released under the MIT License â€“ see the `LICENSE` file for details.

---
